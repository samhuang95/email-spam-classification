# email-spam-classification

This repository provides a baseline spam-classification pipeline and a small Streamlit-based UI for demo and exploration.

What's included

- Training and evaluation scripts under `src/` (baseline SVM pipeline).
- A small Flask prediction API (`src/api.py`) and a Streamlit UI (`streamlit_app.py`).
- Visualization/demo generator and precomputed visuals in `artifacts/`.
- Explainability helpers in `src/explainability.py` (token-removal importance and optional SHAP wrapper).

Quickstart (Windows / PowerShell)

1. Create and activate a virtual environment, then install dependencies:

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1; .venv\Scripts\pip.exe install -r requirements.txt
```

2. (Optional) Add SHAP if you want richer explanations in the UI:

```powershell
.venv\Scripts\pip.exe install shap
```

3. Download the dataset (default path: `data/sms_spam.csv`):

```powershell
.venv\Scripts\python.exe scripts/download_dataset.py --out data/sms_spam.csv
```

4. Train the baseline model (this writes `artifacts/baseline_model.joblib` by default):

```powershell
.venv\Scripts\python.exe -m src.train_baseline --data data/sms_spam.csv --out artifacts/baseline_model.joblib
```

5. Evaluate and produce a report/metrics JSON:

```powershell
.venv\Scripts\python.exe -m src.evaluate --model artifacts/baseline_model.joblib --data data/sms_spam.csv --out-json artifacts/metrics.json --out-md artifacts/report.md
```

6. Start the Flask prediction API (used by the Streamlit app):

```powershell
.venv\Scripts\python.exe -m src.api
```

7. Run the Streamlit demo UI (default port 8501):

```powershell
.venv\Scripts\python.exe -m streamlit run streamlit_app.py --server.headless true
```

Using the Streamlit UI

- Predict tab: enter a message and press Classify. If the local model artifact exists in `artifacts/baseline_model.joblib` or under `models/`, you can enable "Show local explanation" to compute a token-removal importance ranking. If you installed SHAP, the app will also attempt a SHAP-based explanation (may be slow for KernelExplainer).
- Explore Data: preview dataset or view precomputed visual summaries from `artifacts/visuals.json`.
- Evaluation: view metrics from `artifacts/metrics.json` and ROC/PR curves generated by `scripts/generate_visuals.py`.

Artifacts and models

- `artifacts/` — evaluation outputs, visuals and the default model `artifacts/baseline_model.joblib`.
- `models/` — optional directory to place alternate model pipelines (the explainability loader prefers `models/` if present).

Troubleshooting

- If Streamlit can't call the prediction API, ensure `src.api` is running and reachable at `http://127.0.0.1:5000/predict`.
- If the explainability UI reports "no local pipeline found", place your saved sklearn pipeline in `artifacts/baseline_model.joblib` or in `models/` with a supported filename.
- SHAP: KernelExplainer is generic but slow. For faster, more accurate explanations, consider model-specific explainers (e.g. `shap.TreeExplainer` for tree ensembles).

Development notes

- Tests are in `tests/` (a small integration test exists). Use `pytest` to run tests.
- OpenSpec change proposals and project metadata live under `openspec/`.

License & citation

NCHU HW3 - email spam classification
