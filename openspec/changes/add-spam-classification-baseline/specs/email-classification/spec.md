```markdown
## ADDED Requirements

### Requirement: Baseline Spam Classification (phase1)

The system SHALL provide a baseline spam classification pipeline that trains and evaluates a supervised classifier on the provided dataset and produces machine-readable metrics and a human-readable report.

#### Scenario: Train and evaluate baseline model

- **GIVEN** the dataset downloaded from the provided URL and a configuration for training
- **WHEN** the baseline training pipeline is executed (`python -m src.train_baseline --config config/baseline.yaml`)
- **THEN** the pipeline trains a Logistic Regression classifier (and optionally an SVM) on training data
- **AND** writes a metrics JSON file containing precision, recall, F1, and ROC-AUC to the configured output path
- **AND** writes a Markdown report summarizing metrics and confusion matrix to the configured output path

#### Scenario: CI smoke test on fixture data

- **GIVEN** a small fixture dataset committed under `tests/fixtures/` or generated by a download script
- **WHEN** the CI job runs the training/evaluation pipeline on the fixture
- **THEN** the job exits with code 0 and asserts that a JSON metrics file exists

## ADDED Requirements

### Requirement: Evaluation and Report Generation

The system SHALL provide a reproducible evaluation pipeline that computes standard classification metrics (precision, recall, F1, ROC-AUC) and outputs both a machine-readable metrics file (JSON) and a human-readable report (Markdown or HTML).

#### Scenario: Generate evaluation report for a trained model

- **GIVEN** a trained classifier artifact and a held-out test dataset
- **WHEN** the evaluation pipeline is executed with `python -m src.evaluate --config config/eval.yaml`
- **THEN** a JSON metrics file is written to the configured output directory
- **AND** a Markdown (or HTML) report summarizing the metrics and confusion matrix is produced

#### Scenario: CI validation of evaluation run

- **GIVEN** a small fixture dataset included in the repo
- **WHEN** the CI pipeline runs the evaluation script
- **THEN** the script exits with code 0 and produces a metrics JSON file
```
